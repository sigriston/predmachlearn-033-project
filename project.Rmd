---
title: "Automated Recognition of Proper Form in Weightlifiting Exercises"
author: "Thiago Sigrist"
output: html_document
---

```{r loading_libraries, include=FALSE}
library(ggplot2)
library(knitr)
library(doParallel)
registerDoParallel(cores = 8)
library(caret)
library(jsonlite)
library(digest)
```
```{r global_options, include=FALSE}
opts_chunk$set(echo = FALSE,
               message = FALSE,
               warning = FALSE)
set.seed(12345)
```
```{r load_data, include=FALSE}
dataset <- read.csv("data/pml-training.csv.xz")
```

```{r preprocessing, include=FALSE}
naCols <- sapply(dataset, anyNA)
factorCols <- sapply(dataset, is.factor)
ignCols <- naCols | factorCols
useCols <- !ignCols

useCols["X"] <- FALSE
useCols["classe"] <- TRUE
datasetNum <- dataset[,useCols]
```

```{r partition, include=FALSE}
inTrain <- createDataPartition(datasetNum$classe, p = 0.7, list = FALSE)
training <- datasetNum[inTrain,]
testing <- datasetNum[-inTrain,]
```
```{r models, include=FALSE}
models <- list(
  lda = list(),
  qda = list(),
  gbm = list(),
  C5.0 = list(),
  rf = list()
)
```
```{r training, include=FALSE}
trainModels <- function(mlist) {
  for (i in seq_along(mlist)) {
    # add method arg
    mlist[[i]]$method <- names(mlist[i])
    
    # calc JSON MD5
    itemJSON <- toJSON(mlist[[i]])
    itemMD5 <- digest(itemJSON, algo = "md5", serialize = FALSE)

    # add invariate args
    mlist[[i]]$form <- formula("classe ~ .")
    mlist[[i]]$data <- as.data.frame(training)
    
    # set md5 last
    mlist[[i]]$md5 <- itemMD5
  }

  lapply(mlist, function(fa) {
    #take md5 out of fitArgs
    fitArgs <- fa[-length(fa)]
    fitArgs_md5 <- fa$md5
    modf <- file.path("models", paste0("model_", fitArgs_md5, ".rds"))
    if (file.exists(modf)) {
      message(paste0("reading model from '", modf, "'."))
      return(readRDS(modf))
    } else {
      fitMod <- do.call(train.formula, fitArgs)
      saveRDS(fitMod, modf)
      return(fitMod)
    }
  })
}

fitMods <- trainModels(models)
```

```{r testing, include=FALSE}
model_preds <- lapply(fitMods, function(fit) {
  pred <- predict(fit, newdata = testing)
  list(fit = fit, pred = pred)
})
model_summaries <- sapply(model_preds, function(mp) {
  predCM <- confusionMatrix(mp$pred, testing$classe)
  list(Model = mp$fit$modelInfo$label,
       "Training Time (sec)" = mp$fit$times$everything["elapsed"],
       "Training Set Accuracy" = max(mp$fit$results$Accuracy),
       "Test Set Accuracy" = predCM$overall["Accuracy"],
       "Test Acc. Lower Bound" = predCM$overall["AccuracyLower"],
       "Test Acc. Upper Bound" = predCM$overall["AccuracyUpper"])
})
rf_acc <- round(model_summaries[[4, 5]] * 100, 2)
```

## Synopsis

This report investigates whether it's possible to give qualitative assessments
on the proper form of weightlifting exercises, using nothing more than
quantitative measurements (from accelerometers) taken from 6 different
participants.

A machine learning model is then constructed via the Random Forest method that
is shown to be highly accurate, exhibiting **`r rf_acc`%** accuracy on the
validation dataset.

This report was submitted as a course project for the [Practical Machine
Learning] class on Coursera, part of the [Data Science Specialization].

## Experiment and dataset descriptions

The experiment was conducted as follows: 6 young, healthy participants were
asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps
Curl, either in perfect form (class **A**) or simulating one of four common
mistakes (classes **B**-**E**), leading to the 5 classes of execution shown in
the table below:

| Class | Description |
|:-----:| ----------- |
| A | Perfect form |
| B | Throwing the elbows to the front |
| C | Lifting the dumbbell only halfway |
| D | Lowering the dumbbell only halfway |
| E | Throwing the hips to the front |

Motion data was then collected using wearable devices with accelerometers on the
participants' belt, forearm, arm and dumbbell.

The dataset then consists of the `classe` variable, which identifies the class
of form with which the exercise is being performed, plus several variables for
the raw measurements from the devices, several more for aggregates computed on a
time interval/window, and finally, some ancillary variables for miscellaneous
things such as timestamps, participant IDs etc.

## Preprocessing

In this project we utilized the dataset much like a black box, without worrying
about the interpretation of each individual variable. Therefore, some very
minimal preprocessing was applied, just to remove from the dataset the variables
with missing values (mostly those are the aggregate variables, which will only
have values at the end of a time window), or factor and non-numeric variables.

This preprocessing was performed with the R code listed below.

```{r preprocessing, echo=TRUE}
```

## Training

In order to train the machine learning models, the gold standard dataset was
then partitioned into a testing and training dataset, with 70% of the
observations pertaining to the training set and the remaining 30% to the testing
set.

This partitioning was conveniently performed using the `createDataPartition()`
function from the `caret` package, as shown in the R code snippet below.

```{r partition, echo=TRUE}
```

After partitioning, we chose five different machine learning algorithms to apply
to our training dataset. Training was performed with the `train()` function of
the `caret` package in R, using the default options for both preprocessing and
training.

We then obtained metrics from all models, such as training set and test set
accuracy. Running time for training was also computed to give an idea of the
algorithms' efficiency. These metrics are shown on the table below.

```{r testing_results}
kable(t(model_summaries))
```

## Results

Based on the accuracy scores obtained, our choice was the model created with the
Random Forest algorithm, which had the highest score of **`r rf_acc`%** on the
validation dataset (out-of-sample accuracy).

However, an important consideration must be made: the **C5.0** and **Stochastic 
Gradient Boosting** methods also produced impressive accuracy scores, 
particularly the former. It pretty much matched the accuracy of Random Forest 
but its training was much less compute-intensive, taking only a third of the 
compute time that Random Forest did. For this reason, it is quite possible that 
it could be a better choice for real-world applications.

To better understand the results of Random Forest when applied to the validation
dataset, we included below a summary given by the `confusionMatrix()` function.

```{r results}
rf_pred <- model_preds$rf$pred
confusionMatrix(rf_pred, testing$classe)
```

The table above confirms our accuracy scores but shows a few extra details that
are very interesting. Of particular interest to the problem domain are the
results for class **A**, with its great sensitivity and specificity scores
(perfect for the validation dataset). Considering that class A is perfect
exercise form, this is a good result because it's reasonable to assume that
weight trainers are more concerned whether their form is correct or not, and
whatever specific mistake they may be making is probably a secondary concern.

```{r submission_preds}
subm_dataset <- read.csv("data/pml-testing.csv")
subm_pred_C50 <- predict(model_preds$C5.0$fit, subm_dataset)
subm_pred_rf <- predict(model_preds$rf$fit, subm_dataset)
```

```{r submission_files, eval=FALSE}
pml_write_files <- function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(subm_pred_rf)
confusionMatrix(subm_pred_C50, subm_pred_rf, dnn = c("C5.0", "Random Forest"))
```

[Practical Machine Learning]: https://class.coursera.org/predmachlearn-033
[Data Science Specialization]: https://www.coursera.org/specializations/jhudatascience
